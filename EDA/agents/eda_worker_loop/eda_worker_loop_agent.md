# EDA Worker Loop Agent

## Overview
The EDA Worker Loop Agent orchestrates the parallel testing of multiple hypotheses generated by the Hypothesis Generation Agent. It manages an async loop that spawns multiple EDA Analysis Agent instances to test hypotheses concurrently, optimizing for efficiency and resource utilization.

## Agent Specification

### Purpose
- Orchestrate parallel hypothesis testing using async processing
- Manage resource allocation and load balancing
- Coordinate data fetching and sharing between analysis instances
- Aggregate results from multiple EDA Analysis Agent instances
- Handle error recovery and retry logic for failed analyses

### Input Requirements
- **Generated Hypotheses**: List of hypotheses from Hypothesis Generation Agent
- **Target Variable**: The variable being predicted
- **Intugle Knowledge Base**: Semantic layer for data access
- **Resource Constraints**: CPU, memory, and concurrent processing limits
- **Timeout Settings**: Maximum time allowed per hypothesis test

### Core Capabilities

#### 1. Async Task Management
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class EDAWorkerLoopAgent:
    def __init__(self, max_workers=5, timeout_per_hypothesis=300):
        self.max_workers = max_workers
        self.timeout_per_hypothesis = timeout_per_hypothesis
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def process_hypotheses(self, hypotheses: List[Dict], kb, target_variable):
        """Process multiple hypotheses in parallel"""
        
        # Create tasks for each hypothesis
        tasks = []
        for hypothesis in hypotheses:
            task = asyncio.create_task(
                self.test_hypothesis_async(hypothesis, kb, target_variable)
            )
            tasks.append(task)
        
        # Process with timeout and error handling
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Separate successful results from exceptions
        successful_results = []
        failed_hypotheses = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_hypotheses.append({
                    "hypothesis": hypotheses[i],
                    "error": str(result),
                    "retry_count": 0
                })
            else:
                successful_results.append(result)
        
        return {
            "successful_results": successful_results,
            "failed_hypotheses": failed_hypotheses,
            "total_processed": len(hypotheses),
            "success_rate": len(successful_results) / len(hypotheses)
        }
```

#### 2. Resource Management
```python
def manage_resources(self, hypotheses):
    """Manage computational resources and prioritize hypotheses"""
    
    # Prioritize hypotheses by test_priority
    sorted_hypotheses = sorted(
        hypotheses, 
        key=lambda x: x.get("test_priority", 0.5), 
        reverse=True
    )
    
    # Group hypotheses by resource requirements
    resource_groups = {
        "light": [],      # Simple correlations, basic transformations
        "medium": [],     # Complex transformations, aggregations
        "heavy": []       # Multiple interactions, large datasets
    }
    
    for hypothesis in sorted_hypotheses:
        complexity = self.assess_hypothesis_complexity(hypothesis)
        resource_groups[complexity].append(hypothesis)
    
    # Allocate resources based on complexity
    allocation_plan = {
        "light": {"workers": 3, "timeout": 120},
        "medium": {"workers": 2, "timeout": 300}, 
        "heavy": {"workers": 1, "timeout": 600}
    }
    
    return resource_groups, allocation_plan

def assess_hypothesis_complexity(self, hypothesis):
    """Assess computational complexity of hypothesis"""
    
    complexity_score = 0
    
    # Transformation complexity
    transformation = hypothesis["predictor_variable"]["transformation"]
    if transformation in ["polynomial_2", "polynomial_3"]:
        complexity_score += 2
    elif transformation in ["log", "sqrt"]:
        complexity_score += 1
    
    # Aggregation complexity
    aggregation = hypothesis["predictor_variable"]["aggregate_by"]
    if aggregation != "none":
        complexity_score += 1
    
    # Interaction features
    interactions = hypothesis.get("interaction_features", [])
    complexity_score += len(interactions)
    
    # Data size estimation
    data_requirements = hypothesis.get("data_requirements", {})
    if len(data_requirements.get("required_tables", [])) > 3:
        complexity_score += 1
    
    if complexity_score <= 2:
        return "light"
    elif complexity_score <= 4:
        return "medium"
    else:
        return "heavy"
```

#### 3. Data Prefetching and Caching
```python
async def prefetch_common_data(self, hypotheses, kb):
    """Prefetch commonly used data to avoid redundant queries"""
    
    # Identify common data requirements
    common_tables = set()
    common_columns = set()
    
    for hypothesis in hypotheses:
        data_req = hypothesis.get("data_requirements", {})
        common_tables.update(data_req.get("required_tables", []))
        common_columns.update(data_req.get("required_columns", []))
    
    # Prefetch common datasets
    prefetched_data = {}
    
    for table in common_tables:
        try:
            # Use DataProductBuilder to fetch table data
            dp_builder = DataProductBuilder()
            
            # Create minimal ETL for table access
            etl_config = ETLModel(
                name=f"prefetch_{table}",
                fields=[
                    FieldsModel(
                        id=f"{table}.{col}",
                        name=col,
                        category="dimension"
                    ) for col in common_columns if col.startswith(f"{table}.")
                ]
            )
            
            dataset = dp_builder.build(etl_config)
            prefetched_data[table] = dataset.to_df()
            
        except Exception as e:
            print(f"Failed to prefetch {table}: {e}")
    
    return prefetched_data
```

#### 4. Error Handling and Retry Logic
```python
async def test_hypothesis_with_retry(self, hypothesis, kb, target_variable, max_retries=3):
    """Test hypothesis with retry logic for failed attempts"""
    
    for attempt in range(max_retries + 1):
        try:
            # Create EDA Analysis Agent instance
            eda_agent = EDAAnalysisAgent(
                hypothesis=hypothesis,
                kb=kb,
                target_variable=target_variable,
                timeout=self.timeout_per_hypothesis
            )
            
            # Execute analysis with timeout
            result = await asyncio.wait_for(
                eda_agent.analyze_hypothesis(),
                timeout=self.timeout_per_hypothesis
            )
            
            return {
                "hypothesis_id": hypothesis["hypothesis_id"],
                "status": "success",
                "result": result,
                "attempts": attempt + 1,
                "execution_time": result.get("execution_time", 0)
            }
            
        except asyncio.TimeoutError:
            if attempt < max_retries:
                print(f"Timeout for hypothesis {hypothesis['hypothesis_id']}, retrying...")
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
            else:
                return {
                    "hypothesis_id": hypothesis["hypothesis_id"],
                    "status": "timeout",
                    "error": "Analysis timed out after maximum retries",
                    "attempts": max_retries + 1
                }
                
        except Exception as e:
            if attempt < max_retries:
                print(f"Error for hypothesis {hypothesis['hypothesis_id']}: {e}, retrying...")
                await asyncio.sleep(2 ** attempt)
            else:
                return {
                    "hypothesis_id": hypothesis["hypothesis_id"],
                    "status": "error",
                    "error": str(e),
                    "attempts": max_retries + 1
                }
```

### Tools and Dependencies

#### Core Dependencies
- **asyncio**: For async task management
- **concurrent.futures**: For thread pool management
- **Intugle Integration**: DataProductBuilder, KnowledgeBuilder
- **EDA Analysis Agent**: Individual hypothesis testing agent

#### Monitoring and Logging
- **Progress Tracking**: Real-time progress updates
- **Resource Monitoring**: CPU, memory, and I/O usage
- **Error Logging**: Detailed error tracking and reporting
- **Performance Metrics**: Execution times and success rates

### Output Format

```python
{
    "execution_summary": {
        "total_hypotheses": 10,
        "successful_analyses": 8,
        "failed_analyses": 2,
        "success_rate": 0.8,
        "total_execution_time": 1200.5,
        "average_execution_time": 150.1
    },
    "results": [
        {
            "hypothesis_id": "hyp_1",
            "status": "success",
            "result": {
                "correlation_score": 0.75,
                "p_value": 0.001,
                "visualization_path": "/charts/hyp_1_correlation.png",
                "data_summary": {...},
                "transformation_applied": "log",
                "execution_time": 145.2
            },
            "attempts": 1
        },
        {
            "hypothesis_id": "hyp_2", 
            "status": "error",
            "error": "Data not available for required tables",
            "attempts": 3
        }
    ],
    "resource_utilization": {
        "peak_cpu_usage": 85.2,
        "peak_memory_usage": 2048,
        "total_data_processed": "2.5GB",
        "cache_hit_rate": 0.75
    },
    "failed_hypotheses": [
        {
            "hypothesis_id": "hyp_2",
            "error_type": "data_unavailable",
            "error_message": "Required table 'customer_satisfaction' not found",
            "retry_attempts": 3,
            "recommendation": "Verify data schema or skip hypothesis"
        }
    ],
    "performance_insights": [
        "Log transformations showed highest correlation scores",
        "Temporal aggregations required most computational resources",
        "Interaction features had mixed results",
        "Data prefetching improved performance by 25%"
    ]
}
```

### Agent Behavior

#### 1. Initialization
- Set up async task management infrastructure
- Configure resource limits and timeouts
- Initialize monitoring and logging systems
- Prepare data prefetching strategies

#### 2. Hypothesis Processing
- Prioritize hypotheses by test priority
- Group hypotheses by computational complexity
- Allocate resources based on complexity assessment
- Prefetch commonly used data

#### 3. Parallel Execution
- Spawn EDA Analysis Agent instances for each hypothesis
- Monitor resource utilization and performance
- Handle timeouts and errors gracefully
- Implement retry logic for failed analyses

#### 4. Result Aggregation
- Collect results from all analysis instances
- Separate successful from failed analyses
- Generate performance metrics and insights
- Prepare comprehensive summary for downstream agents

### Error Handling
- Handle individual hypothesis failures without stopping the entire process
- Implement exponential backoff for retries
- Manage resource exhaustion gracefully
- Provide detailed error reporting and recommendations

### Performance Considerations
- Use async processing for I/O-bound operations
- Implement data caching to avoid redundant queries
- Balance parallel processing with resource constraints
- Monitor and optimize memory usage

### Integration Points
- **Input**: Receives hypotheses from Hypothesis Generation Agent
- **Output**: Provides aggregated analysis results to Summarizer Agent
- **Tools**: Orchestrates multiple EDA Analysis Agent instances
- **State**: Updates workflow state with execution progress and results

### Example Usage
```python
# Initialize worker loop agent
worker_agent = EDAWorkerLoopAgent(
    max_workers=5,
    timeout_per_hypothesis=300
)

# Process hypotheses in parallel
results = await worker_agent.process_hypotheses(
    hypotheses=generated_hypotheses,
    kb=knowledge_builder,
    target_variable="sales_revenue"
)

# Access results
successful_results = results["successful_results"]
failed_hypotheses = results["failed_hypotheses"]
performance_metrics = results["execution_summary"]
```

### Success Criteria
- Successfully process at least 80% of generated hypotheses
- Complete analysis within reasonable time limits
- Provide detailed error reporting for failed hypotheses
- Generate comprehensive performance metrics
- Maintain system stability under concurrent load
